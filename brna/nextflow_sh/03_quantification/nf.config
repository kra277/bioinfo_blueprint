// nextflow.config
// This script must be run after downloading and QCing the fastq files
// place the required main.nf, nf.config, run_nf.sh in scripts/03_quantification dir 
// execute the run_nf_local or run_nf_hpc from the scripts/03_quantification dir
// after running the nextflow, 
// quant_results will have all the fastp results, STAR aligned BAMs, 
// generated counts, and multiqc results

params {
  // Note: Container, main_dir and gse variables were set in execution script
  main_dir       = System.getenv("NF_MAIN_DIR")
  gse            = System.getenv("NF_GSE") 
  sif            = null

  // Important variables - please edit as needed
  org_name       = "Mus_musculus"      // organism name 
  build          = "GRCm39"            // genome build
  release        = "115"               // genome release version

  // Paths
  proj_dir       = "${params.main_dir}/${params.gse}"
  data           = "${params.proj_dir}/data"
  fastq_dir      = "${params.proj_dir}/raw_fastq"
  reads          = "${params.fastq_dir}/*_{1,2}.fastq.gz"
  log_dir        = "${params.proj_dir}/scripts/03_quantification/log"

  res_dir        = "${params.proj_dir}/quant_results"
  fastp_res      = "${params.res_dir}/fastp_res"
  star_res       = "${params.res_dir}/star_res"
  feat_counts    = "${params.res_dir}/feature_counts"
  multiqc_res    = "${params.res_dir}/assorted_log_res"    

  index          = "${params.data}/star_index/"
  gtf_file       = "${params.index}/${org_name}.${build}.${release}.gtf"
           
  star_multimap  = 1                              
  star_mismatch  = 2
}

// -------- Container settings --------
singularity {
  enabled    = true
  autoMounts = true
}

// -------- LOCAL and HPC Profiles --------
profiles {

  // Workstation / laptop
  // set the bind path to your volume
  local {
    executor {name = 'local'}
    process {containerOptions = "--bind /mnt/myvolume"}
  }

  // SLURM HPC
  // queue size can be adjusted as needed
  // clusterOptions is specific to NYU Torch HPC
  // Set the bind path to appropriate scratch location
  hpc {
    executor {name = 'slurm'; queueSize = 50}

    process {
      scratch = true
      clusterOptions = '--account=torch_pr_505_general'
      containerOptions = "--bind /scratch"
    }
  }
}

process {
  // global defaults
  cpus          = 4
  memory        = '8 GB'
  time          = '60m'
  errorStrategy = 'retry'
  maxRetries    = 2
  container     = params.sif
  scratch       = true

  // override as needed
  withName: run_multiqc {cpus   = 2; memory = '4 GB'; time   = '30m'}
  withLabel: big_mem { cpus = 16; memory = '48 GB'}
  withLabel: counts  { cpus = 8;  memory = '16 GB'}

}

// -------- Paper trail --------
def trace_date = new java.util.Date().format('yyyy_MM_dd')

report {
  enabled   = true
  file      = "${params.log_dir}/report_${trace_date}.html"
  overwrite = true
}
timeline {
  enabled   = true
  file      = "${params.log_dir}/timeline_${trace_date}.html"
  overwrite = true
}
trace {
  enabled   = true
  file      = "${params.log_dir}/trace_${trace_date}.txt"
  overwrite = true
}